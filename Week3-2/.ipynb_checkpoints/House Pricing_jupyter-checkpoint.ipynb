{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle House Price 데이터 실습\n",
    "> ### <진행 순서>\n",
    "> #### 1. Data Loading\n",
    "> #### 2. EDA\n",
    "> #### 3. Data Preprocessing\n",
    "> #### 4. Model Fitting \n",
    "> #### 5. Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mydir = os.getcwd() + '/train.csv'\n",
    "\n",
    "housetrn = pd.read_csv(mydir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가져온 자료의 shape 확인 -> 81개의 칼럼과 1460개의 행이 보인다.\n",
    "housetrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리컬에 해당하는 변수 확인해보기 -> 더미변수화 필요\n",
    "housetrn.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 칼럼 확인 많은 변수가 보인다. \n",
    "# let us check the columsn that are part of the input file\n",
    "print (housetrn.columns)\n",
    "# Because the number of columns are large, we can set to display all columns\n",
    "pd.options.display.max_columns = 999\n",
    "print (pd.options.display.max_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 상단만을 가져와 확인해보자.\n",
    "# 카테고리 및 연속형 칼럼들이 보인다.\n",
    "# 불필요한 index , NaN등도 삭제할 필요성이 있겠다.\n",
    "print (housetrn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn을 사용한 데이터 분포 시각화\n",
    "# https://datascienceschool.net/view-notebook/4c2d5ff1caab4b21a708cc662137bc65/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음은 모든 피쳐중 하나와 y와의 스캐터플롯을 각각 그려본 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설명변수와 예측변수간의 stripplot을 그려보자.\n",
    "# Let us do a scatterplot for the variable selected as HIGH\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style('ticks')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "sns.stripplot(x=\"Neighborhood\", y=\"SalePrice\", data=housetrn, jitter=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood에 따라 y축의 Price 분포가 상이\n",
    "집 값을 떨어뜨리는 혹은 높이는 변수가 존재함을 유추할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housetrn['BldgType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OverallQual와 BldgType , price 간의 factorplot\n",
    "# factorplot은 색상(hue)과 행(row) 등을 동시에 사용하여 3 개 이상의 카테고리 값에 의한 분포 변화를 보여준다.\n",
    "sns.set_style('ticks')\n",
    "sns.factorplot(x=\"OverallQual\", y=\"SalePrice\", col=\"BldgType\", data=housetrn, kind=\"swarm\", col_wrap=3)\n",
    "fig.set_size_inches(5, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 년도와 price간의 barplot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "sns.barplot(x=\"YearBuilt\", y=\"SalePrice\", data=housetrn)\n",
    "plt.xticks(rotation=90)\n",
    "sns.set_style('ticks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TotalBsmtSF와 price간의 regplot\n",
    "## GrLivArea와 price간의 regplot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "sns.regplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=housetrn)\n",
    "sns.regplot(x=\"GrLivArea\", y=\"SalePrice\", data=housetrn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SaleCondition와 price 간의 factorplot\n",
    "sns.factorplot(x=\"SaleCondition\", y=\"SalePrice\", col=\"Functional\", data=housetrn, kind=\"swarm\", col_wrap=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수간의 correlation 확인\n",
    "# Plotting the Pearson correlation of the different features\n",
    "corr_matrix = housetrn.corr()\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(corr_matrix,linewidths=0.1,vmax=0.8, square=True, cmap=colormap, linecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 노란색일수록 양의 상관 남색일수록 음의 상관이 강하다.\n",
    "- 몇몇 변수간의 뚜렷한 양/음의 상관이 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의미없다고 판단되는 변수는 빼고 분석하자. \n",
    "#newhousetrn = housetrn\n",
    "newhousetrn = housetrn[['Id', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'OverallQual', 'TotalBsmtSF', '1stFlrSF',  'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars','GarageArea', 'Neighborhood', 'YearBuilt', 'YearRemodAdd','Functional', 'SalePrice']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (newhousetrn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values 확인 \n",
    "total = newhousetrn.isnull().sum().sort_values(ascending=False)\n",
    "percent = (newhousetrn.isnull().sum()/newhousetrn.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print (missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotFrontage, GarageYrBlt, MasVnrArea 세 변수에 대해 missing values처리가 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LotFrontage 변수 미싱밸류 처리\n",
    "# mean +- std 를 랜덤으로 집어넣기\n",
    "lot_av = newhousetrn.LotFrontage.mean()\n",
    "lot_sd = newhousetrn.LotFrontage.std()\n",
    "tot_mislot = newhousetrn.LotFrontage.isnull().sum()\n",
    "rand_lot= np.random.randint(lot_av - lot_sd, lot_av + lot_sd, size=tot_mislot)\n",
    "newhousetrn['LotFrontage'][np.isnan(newhousetrn['LotFrontage'])] = rand_lot\n",
    "newhousetrn['LotFrontage'] = newhousetrn['LotFrontage'].astype(int)\n",
    "newhousetrn.LotFrontage.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MasVnrArea에 대해 같은 작업 반복\n",
    "mva_av = newhousetrn.MasVnrArea.mean()\n",
    "mva_sd = newhousetrn.MasVnrArea.std()\n",
    "tot_mismva = newhousetrn.MasVnrArea.isnull().sum()\n",
    "rand_mva= np.random.randint(mva_av - mva_sd, mva_av + mva_sd, size=tot_mismva)\n",
    "newhousetrn['MasVnrArea'][np.isnan(newhousetrn['MasVnrArea'])] = rand_mva\n",
    "newhousetrn['MasVnrArea'] = newhousetrn['MasVnrArea'].astype(int)\n",
    "newhousetrn.MasVnrArea.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GarageYrBlt에도 같은 작업 반복 \n",
    "gyr_av = newhousetrn.GarageYrBlt.mean()\n",
    "gyr_sd = newhousetrn.GarageYrBlt.std()\n",
    "tot_misgyr = newhousetrn.GarageYrBlt.isnull().sum()\n",
    "rand_gyr= np.random.randint(gyr_av - gyr_sd, gyr_av + gyr_sd, size=tot_misgyr)\n",
    "newhousetrn['GarageYrBlt'][np.isnan(newhousetrn['GarageYrBlt'])] = rand_gyr\n",
    "newhousetrn['GarageYrBlt'] = newhousetrn['GarageYrBlt'].astype(int)\n",
    "newhousetrn.GarageYrBlt.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values처리 확인\n",
    "total = newhousetrn.isnull().sum().sort_values(ascending=False)\n",
    "percent = (newhousetrn.isnull().sum()/newhousetrn.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "print (missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터타입 중 카테고리형(object)을 바꿀 필요성이 있다. \n",
    "newhousetrn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the Categorical variables\n",
    "cols_to_transform = newhousetrn[['Id', 'Neighborhood', 'Functional']]\n",
    "newcols = pd.get_dummies(cols_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 칼럼 제거\n",
    "del newhousetrn['Neighborhood'] # 또는 newhousetrn = newhousetrn.drop(labels=[\"Neighborhood\"], axis=1)\n",
    "\n",
    "del newhousetrn['Functional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더미변수화 한 열을 join\n",
    "fhoustrn = newhousetrn.merge(newcols, how='inner', on='Id' )\n",
    "fhoustrn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhoustrn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = housetrn.SalePrice\n",
    "#del fhoustrn['SalePrice']\n",
    "X = fhoustrn[:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "std_scale = preprocessing.StandardScaler().fit(y_train.reshape(-1,1))\n",
    "y_train = std_scale.transform(y_train.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler이용 \n",
    "# Standardizing for the unscaled 15 variables in X_train\n",
    "from sklearn import preprocessing\n",
    "std_scale1 = preprocessing.StandardScaler().fit(X_train[['OverallQual']])\n",
    "X_train[['OverallQual']] = std_scale1.transform(X_train[['OverallQual']])\n",
    "std_scale2 = preprocessing.StandardScaler().fit(X_train[['TotalBsmtSF']])\n",
    "X_train[['TotalBsmtSF']] = std_scale2.transform(X_train[['TotalBsmtSF']])\n",
    "std_scale3 = preprocessing.StandardScaler().fit(X_train[['1stFlrSF']])\n",
    "X_train[['1stFlrSF']] = std_scale3.transform(X_train[['1stFlrSF']])\n",
    "std_scale4 = preprocessing.StandardScaler().fit(X_train[['GrLivArea']])\n",
    "X_train[['GrLivArea']] = std_scale4.transform(X_train[['GrLivArea']])\n",
    "std_scale5 = preprocessing.StandardScaler().fit(X_train[['FullBath']])\n",
    "X_train[['FullBath']] = std_scale5.transform(X_train[['FullBath']])\n",
    "std_scale6 = preprocessing.StandardScaler().fit(X_train[['GarageArea']])\n",
    "X_train[['GarageArea']] = std_scale6.transform(X_train[['GarageArea']])\n",
    "std_scale7 = preprocessing.StandardScaler().fit(X_train[['YearBuilt']])\n",
    "X_train[['YearBuilt']] = std_scale7.transform(X_train[['YearBuilt']])\n",
    "std_scale8 = preprocessing.StandardScaler().fit(X_train[['LotFrontage']])\n",
    "X_train[['LotFrontage']] = std_scale8.transform(X_train[['LotFrontage']])\n",
    "std_scale9 = preprocessing.StandardScaler().fit(X_train[['MasVnrArea']])\n",
    "X_train[['MasVnrArea']] = std_scale9.transform(X_train[['MasVnrArea']])\n",
    "std_scale10 = preprocessing.StandardScaler().fit(X_train[['BsmtFinSF1']])\n",
    "X_train[['BsmtFinSF1']] = std_scale10.transform(X_train[['BsmtFinSF1']])\n",
    "std_scale11 = preprocessing.StandardScaler().fit(X_train[['TotRmsAbvGrd']])\n",
    "X_train[['TotRmsAbvGrd']] = std_scale11.transform(X_train[['TotRmsAbvGrd']])\n",
    "std_scale12 = preprocessing.StandardScaler().fit(X_train[['Fireplaces']])\n",
    "X_train[['TotRmsAbvGrd']] = std_scale12.transform(X_train[['Fireplaces']])\n",
    "std_scale13 = preprocessing.StandardScaler().fit(X_train[['GarageYrBlt']])\n",
    "X_train[['GarageYrBlt']] = std_scale13.transform(X_train[['GarageYrBlt']])\n",
    "std_scale14 = preprocessing.StandardScaler().fit(X_train[['GarageCars']])\n",
    "X_train[['GarageCars']] = std_scale14.transform(X_train[['GarageCars']])\n",
    "std_scale15 = preprocessing.StandardScaler().fit(X_train[['YearRemodAdd']])\n",
    "X_train[['YearRemodAdd']] = std_scale15.transform(X_train[['YearRemodAdd']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us apply the same technique of rescaling for the Test data set too\n",
    "std_scale0 = preprocessing.StandardScaler().fit(y_test.reshape(-1,1))\n",
    "y_test = std_scale0.transform(y_test.reshape(-1,1))\n",
    "\n",
    "std_scale16 = preprocessing.StandardScaler().fit(X_test[['OverallQual']])\n",
    "X_test[['OverallQual']] = std_scale16.transform(X_test[['OverallQual']])\n",
    "std_scale17 = preprocessing.StandardScaler().fit(X_test[['TotalBsmtSF']])\n",
    "X_test[['TotalBsmtSF']] = std_scale17.transform(X_test[['TotalBsmtSF']])\n",
    "std_scale18 = preprocessing.StandardScaler().fit(X_test[['1stFlrSF']])\n",
    "X_test[['1stFlrSF']] = std_scale18.transform(X_test[['1stFlrSF']])\n",
    "std_scale19 = preprocessing.StandardScaler().fit(X_test[['GrLivArea']])\n",
    "X_test[['GrLivArea']] = std_scale19.transform(X_test[['GrLivArea']])\n",
    "std_scale20 = preprocessing.StandardScaler().fit(X_test[['FullBath']])\n",
    "X_test[['FullBath']] = std_scale20.transform(X_test[['FullBath']])\n",
    "std_scale21 = preprocessing.StandardScaler().fit(X_test[['GarageArea']])\n",
    "X_test[['GarageArea']] = std_scale21.transform(X_test[['GarageArea']])\n",
    "std_scale22 = preprocessing.StandardScaler().fit(X_test[['YearBuilt']])\n",
    "X_test[['YearBuilt']] = std_scale22.transform(X_test[['YearBuilt']])\n",
    "std_scale23 = preprocessing.StandardScaler().fit(X_test[['LotFrontage']])\n",
    "X_test[['LotFrontage']] = std_scale23.transform(X_test[['LotFrontage']])\n",
    "std_scale24 = preprocessing.StandardScaler().fit(X_test[['MasVnrArea']])\n",
    "X_test[['MasVnrArea']] = std_scale24.transform(X_test[['MasVnrArea']])\n",
    "std_scale25 = preprocessing.StandardScaler().fit(X_test[['BsmtFinSF1']])\n",
    "X_test[['BsmtFinSF1']] = std_scale25.transform(X_test[['BsmtFinSF1']])\n",
    "std_scale26 = preprocessing.StandardScaler().fit(X_test[['TotRmsAbvGrd']])\n",
    "X_test[['TotRmsAbvGrd']] = std_scale26.transform(X_test[['TotRmsAbvGrd']])\n",
    "std_scale27 = preprocessing.StandardScaler().fit(X_test[['Fireplaces']])\n",
    "X_test[['TotRmsAbvGrd']] = std_scale27.transform(X_test[['Fireplaces']])\n",
    "std_scale28 = preprocessing.StandardScaler().fit(X_test[['GarageYrBlt']])\n",
    "X_test[['GarageYrBlt']] = std_scale28.transform(X_test[['GarageYrBlt']])\n",
    "std_scale29 = preprocessing.StandardScaler().fit(X_test[['GarageCars']])\n",
    "X_test[['GarageCars']] = std_scale29.transform(X_test[['GarageCars']])\n",
    "std_scale30 = preprocessing.StandardScaler().fit(X_test[['YearRemodAdd']])\n",
    "X_test[['YearRemodAdd']] = std_scale30.transform(X_test[['YearRemodAdd']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 일반회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression()\n",
    "rst = model1.fit(X_train, y_train)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 릿지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Grid Search with specific alpha values for Ridge\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001])\n",
    "#alphas = np.logspace(-6, 6, 100)\n",
    "model2 = Ridge()\n",
    "grid_Ridge = GridSearchCV(model2, cv = 10,  param_grid = dict(alpha=alphas))\n",
    "grid_Ridge.fit(X_train, y_train)\n",
    "print(grid_Ridge)\n",
    "# summarize the results of the grid search\n",
    "print(grid_Ridge.best_score_)\n",
    "print(grid_Ridge.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 라쏘 해보기(직접)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Grid Search with specific of alpha values for Lasso\n",
    "alphas =\n",
    "\n",
    "\n",
    "\n",
    "model3 =\n",
    "grid_Lasso = GridSearchCV(estimator=, cv = , param_grid=)\n",
    "grid_Lasso.fit(, )\n",
    "print(grid_Lasso)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid_Lasso.best_score_)\n",
    "print(grid_Lasso.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Grid Search specific alpha values for ElasticNet\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001])\n",
    "#alphas = np.logspace(-6, 6, 100)\n",
    "\n",
    "# param_grid = {'alpha': sp_rand(), 'l1_ratio': sp_rand()}\n",
    "model4 = ElasticNet()\n",
    "grid_ELN = GridSearchCV(estimator=model4, param_grid=dict(alpha=alphas))\n",
    "# rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "grid_ELN.fit(X_train, y_train)\n",
    "print(grid_ELN)\n",
    "# summarize the results of the grid search\n",
    "print(grid_ELN.best_score_)\n",
    "print(grid_ELN.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#mean_squared_error(y_true, y_pred)\n",
    "from sklearn.metrics import r2_score\n",
    "# r2_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test,grid_Ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test,grid_Lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test,grid_ELN.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
