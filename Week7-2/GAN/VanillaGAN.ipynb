{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by yunjey/pytorch-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 실제 이미지들과 fake 이미지들을 샘플링합니다.\n",
    "\n",
    "\n",
    "실제 이미지는 데이터셋에서 Load합니다.\n",
    "fake 이미지는 Generator에 noise라는 인풋을 넣어서 만듭니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . Discriminator를 학습시킵니다.\n",
    "\n",
    "(1) 실제 이미지들을 넣고 분류기를 돌려봅니다.\n",
    "\n",
    "real_loss: 실제 이미지들을 넣은 결과값들(0 혹은 1로 구성된 벡터)와 실제 이미지들의 레이블(1로 이루어진 벡터)를 비교해서 계산된 loss\n",
    "\n",
    "(2) fake 이미지들을 넣고 분류기를 돌려봅니다.\n",
    "\n",
    "fake_loss: fake 이미지들을 넣은 결과값(0 혹은 1로 구성된 벡터)와 fake 이미지들의 레이블(영벡터)를 비교해서 계산된 loss\n",
    "\n",
    "(3) Discriminator's loss = real_loss + fake_loss\n",
    "\n",
    "\n",
    "(4) 오차 역전파 및 파라미터 업데이트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Generator를 학습시킵니다.\n",
    "\n",
    "\n",
    "(1) 새로운 fake 이미지들을 뽑아서 Discriminator에 일종의 테스트 셋으로 넣어봅니다.\n",
    "\n",
    "fake 이미지는 역시 Generator에 noise를 넣어서 만듭니다.\n",
    "\n",
    "(2) 테스트 결과값과 실제 이미지의 레이블을 비교해 loss를 계산합니다.\n",
    "\n",
    "\n",
    "(3) 오차 역전파 및 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Generator가 만든 fake 이미지를 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. 모듈 불러오기 및 환경세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Device configuration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 하이퍼 파라미터 지정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "latent_size = 64 # z size , input size\n",
    "hidden_size = 256\n",
    "image_size = 784 # 28 * 28\n",
    "num_epochs = 20 # 트레인 돌릴 에폭수 \n",
    "batch_size = 100 # 배치 사이즈\n",
    "sample_dir = 'samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                  std=(0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# dataset, 배치사이즈 , one-hot 여부 등등 지정한다.\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. D,G network 정의 loss 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),   #  size가 64인 noise를 인풋으로 받는다, 가중치 벡터를 곱해 256차원 벡터로 확장\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size), # real image size만큼 shape 맞춘다. \n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setting\n",
    "D = D.to(device)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss() # -> class real : 1 , fake :0\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad 초기화 함수\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20], Step [200/600], d_loss: 0.0475, g_loss: 3.9757, D(x): 0.99, D(G(z)): 0.04\n",
      "Epoch [0/20], Step [400/600], d_loss: 0.0612, g_loss: 5.2755, D(x): 0.99, D(G(z)): 0.05\n",
      "Epoch [0/20], Step [600/600], d_loss: 0.0546, g_loss: 5.4928, D(x): 0.98, D(G(z)): 0.03\n",
      "Epoch [1/20], Step [200/600], d_loss: 0.0807, g_loss: 4.7682, D(x): 0.99, D(G(z)): 0.06\n",
      "Epoch [1/20], Step [400/600], d_loss: 0.2162, g_loss: 3.5416, D(x): 0.99, D(G(z)): 0.17\n",
      "Epoch [1/20], Step [600/600], d_loss: 0.5069, g_loss: 5.3590, D(x): 0.87, D(G(z)): 0.18\n",
      "Epoch [2/20], Step [200/600], d_loss: 0.6882, g_loss: 3.3532, D(x): 0.75, D(G(z)): 0.07\n",
      "Epoch [2/20], Step [400/600], d_loss: 0.3443, g_loss: 3.2709, D(x): 0.90, D(G(z)): 0.17\n",
      "Epoch [2/20], Step [600/600], d_loss: 0.8237, g_loss: 2.0928, D(x): 0.70, D(G(z)): 0.22\n",
      "Epoch [3/20], Step [200/600], d_loss: 0.3467, g_loss: 2.8760, D(x): 0.88, D(G(z)): 0.13\n",
      "Epoch [3/20], Step [400/600], d_loss: 0.6924, g_loss: 2.4846, D(x): 0.87, D(G(z)): 0.30\n",
      "Epoch [3/20], Step [600/600], d_loss: 0.3893, g_loss: 3.6759, D(x): 0.87, D(G(z)): 0.13\n",
      "Epoch [4/20], Step [200/600], d_loss: 0.4645, g_loss: 2.0676, D(x): 0.79, D(G(z)): 0.09\n",
      "Epoch [4/20], Step [400/600], d_loss: 0.2988, g_loss: 3.0978, D(x): 0.93, D(G(z)): 0.16\n",
      "Epoch [4/20], Step [600/600], d_loss: 0.3813, g_loss: 4.0939, D(x): 0.88, D(G(z)): 0.08\n",
      "Epoch [5/20], Step [200/600], d_loss: 0.2112, g_loss: 5.1341, D(x): 0.94, D(G(z)): 0.10\n",
      "Epoch [5/20], Step [400/600], d_loss: 0.3261, g_loss: 3.8790, D(x): 0.89, D(G(z)): 0.07\n",
      "Epoch [5/20], Step [600/600], d_loss: 0.3782, g_loss: 3.2531, D(x): 0.86, D(G(z)): 0.06\n",
      "Epoch [6/20], Step [200/600], d_loss: 0.3057, g_loss: 3.3032, D(x): 0.89, D(G(z)): 0.06\n",
      "Epoch [6/20], Step [400/600], d_loss: 0.3339, g_loss: 3.4964, D(x): 0.94, D(G(z)): 0.17\n",
      "Epoch [6/20], Step [600/600], d_loss: 0.3964, g_loss: 2.9709, D(x): 0.90, D(G(z)): 0.17\n",
      "Epoch [7/20], Step [200/600], d_loss: 0.0805, g_loss: 4.7267, D(x): 0.97, D(G(z)): 0.03\n",
      "Epoch [7/20], Step [400/600], d_loss: 0.1267, g_loss: 3.5255, D(x): 0.94, D(G(z)): 0.03\n",
      "Epoch [7/20], Step [600/600], d_loss: 0.1518, g_loss: 4.5137, D(x): 0.95, D(G(z)): 0.06\n",
      "Epoch [8/20], Step [200/600], d_loss: 0.3316, g_loss: 4.4880, D(x): 0.94, D(G(z)): 0.12\n",
      "Epoch [8/20], Step [400/600], d_loss: 0.1506, g_loss: 4.4759, D(x): 0.98, D(G(z)): 0.09\n",
      "Epoch [8/20], Step [600/600], d_loss: 0.2052, g_loss: 3.7179, D(x): 0.94, D(G(z)): 0.07\n",
      "Epoch [9/20], Step [200/600], d_loss: 0.0981, g_loss: 5.6297, D(x): 0.97, D(G(z)): 0.04\n",
      "Epoch [9/20], Step [400/600], d_loss: 0.2080, g_loss: 4.3841, D(x): 0.98, D(G(z)): 0.13\n",
      "Epoch [9/20], Step [600/600], d_loss: 0.2078, g_loss: 6.0748, D(x): 0.92, D(G(z)): 0.01\n",
      "Epoch [10/20], Step [200/600], d_loss: 0.1804, g_loss: 6.9892, D(x): 0.91, D(G(z)): 0.01\n",
      "Epoch [10/20], Step [400/600], d_loss: 0.1161, g_loss: 6.5195, D(x): 0.97, D(G(z)): 0.05\n",
      "Epoch [10/20], Step [600/600], d_loss: 0.1669, g_loss: 5.8263, D(x): 0.95, D(G(z)): 0.05\n",
      "Epoch [11/20], Step [200/600], d_loss: 0.1776, g_loss: 3.4375, D(x): 0.95, D(G(z)): 0.04\n",
      "Epoch [11/20], Step [400/600], d_loss: 0.1617, g_loss: 6.3714, D(x): 0.92, D(G(z)): 0.01\n",
      "Epoch [11/20], Step [600/600], d_loss: 0.1543, g_loss: 4.0220, D(x): 0.95, D(G(z)): 0.05\n",
      "Epoch [12/20], Step [200/600], d_loss: 0.3397, g_loss: 6.4846, D(x): 0.89, D(G(z)): 0.01\n",
      "Epoch [12/20], Step [400/600], d_loss: 0.2460, g_loss: 6.7973, D(x): 0.93, D(G(z)): 0.02\n",
      "Epoch [12/20], Step [600/600], d_loss: 0.0401, g_loss: 9.4930, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [13/20], Step [200/600], d_loss: 0.1281, g_loss: 6.5338, D(x): 0.95, D(G(z)): 0.02\n",
      "Epoch [13/20], Step [400/600], d_loss: 0.2831, g_loss: 4.9163, D(x): 0.91, D(G(z)): 0.04\n",
      "Epoch [13/20], Step [600/600], d_loss: 0.0651, g_loss: 5.7178, D(x): 0.99, D(G(z)): 0.04\n",
      "Epoch [14/20], Step [200/600], d_loss: 0.2350, g_loss: 6.0467, D(x): 0.98, D(G(z)): 0.13\n",
      "Epoch [14/20], Step [400/600], d_loss: 0.2686, g_loss: 4.5231, D(x): 0.96, D(G(z)): 0.14\n",
      "Epoch [14/20], Step [600/600], d_loss: 0.1928, g_loss: 3.9634, D(x): 0.93, D(G(z)): 0.06\n",
      "Epoch [15/20], Step [200/600], d_loss: 0.6586, g_loss: 4.5066, D(x): 0.90, D(G(z)): 0.20\n",
      "Epoch [15/20], Step [400/600], d_loss: 0.1651, g_loss: 6.5596, D(x): 0.93, D(G(z)): 0.01\n",
      "Epoch [15/20], Step [600/600], d_loss: 0.2637, g_loss: 4.3150, D(x): 0.96, D(G(z)): 0.12\n",
      "Epoch [16/20], Step [200/600], d_loss: 0.3295, g_loss: 4.1586, D(x): 0.88, D(G(z)): 0.05\n",
      "Epoch [16/20], Step [400/600], d_loss: 0.3498, g_loss: 3.0267, D(x): 0.91, D(G(z)): 0.10\n",
      "Epoch [16/20], Step [600/600], d_loss: 0.3397, g_loss: 3.2337, D(x): 0.95, D(G(z)): 0.18\n",
      "Epoch [17/20], Step [200/600], d_loss: 0.3136, g_loss: 4.0660, D(x): 0.91, D(G(z)): 0.13\n",
      "Epoch [17/20], Step [400/600], d_loss: 0.3045, g_loss: 4.2115, D(x): 0.90, D(G(z)): 0.09\n",
      "Epoch [17/20], Step [600/600], d_loss: 0.2674, g_loss: 3.4719, D(x): 0.93, D(G(z)): 0.09\n",
      "Epoch [18/20], Step [200/600], d_loss: 0.3694, g_loss: 3.5780, D(x): 0.87, D(G(z)): 0.11\n",
      "Epoch [18/20], Step [400/600], d_loss: 0.2305, g_loss: 3.6727, D(x): 0.93, D(G(z)): 0.08\n",
      "Epoch [18/20], Step [600/600], d_loss: 0.2060, g_loss: 5.6774, D(x): 0.92, D(G(z)): 0.03\n",
      "Epoch [19/20], Step [200/600], d_loss: 0.1999, g_loss: 4.2956, D(x): 0.94, D(G(z)): 0.07\n",
      "Epoch [19/20], Step [400/600], d_loss: 0.2556, g_loss: 3.4766, D(x): 0.92, D(G(z)): 0.07\n",
      "Epoch [19/20], Step [600/600], d_loss: 0.2515, g_loss: 3.6289, D(x): 0.94, D(G(z)): 0.07\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "total_step = len(data_loader) # data_loader 길이 만큼 step_size 지정 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        images = images.reshape(batch_size, -1).to(device) # cpu or gpu \n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "        \n",
    "        # D의 학습은 binary cross entopy 를 통해 진행\n",
    "        \n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels) # loss 계산 (criterion = nn.BCELoss())\n",
    "        real_score = outputs # real 이미지를 넣었을때의 스코어\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "\n",
    "        # G의 인풋 공간  latent space 크기 만큼 노이즈 z 를 생성\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        \n",
    "        # 노이즈 z 를 G에 넣어 페이크 이미지 생성\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # D 가 페이크 이미지를 판단한 결과\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # 로스 계산\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        \n",
    "        # 위에서 D가 페이크 이미지에 대한 스코어 계산한 outputs 이 fake image 에 대한 스코어(0~1) \n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        reset_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step() # back prob 결과를 업데이트 \n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        # G의 인풋 공간  latent space 크기 만큼 노이즈 z 를 생성\n",
    "\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        \n",
    "        # G에 z를 넣어 페이크 이미지 생성\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # 페이크 이미지를 판별\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # G는 크로스 엔트로피 loss 계산 \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        reset_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    # 첫 에폭에서 real image 경로에 저장\n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # 그 다음 에폭 돌때마다 fake image save\n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "\n",
    "# 모델 저장 ckpt or pickle \n",
    "# Save the model checkpoints \n",
    "torch.save(G.state_dict(), 'G.ckpt')\n",
    "torch.save(D.state_dict(), 'D.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
